# =================================================================
# FILE: docker-compose.yml
# (This is the updated version of your existing file, now including the new llm-service)
# =================================================================
version: '3.8'

services:
  db:
    image: postgres:16-alpine
    container_name: noda_turbo_postgres
    restart: always
    ports:
      - "${POSTGRES_PORT}:5432"
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: noda_turbo_redis
    restart: always
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  ingestor:
    build:
      context: .
      dockerfile: ./apps/data-ingestion-service/Dockerfile
    container_name: noda_turbo_ingestor
    depends_on:
      db:
        condition: service_healthy
    restart: on-failure
    environment:
      DATABASE_URL: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}"
      NODE_ENV: development
    volumes:
      - ./apps/data-ingestion-service/data:/app/apps/data-ingestion-service/data

  graphql-api:
    build:
      context: .
      dockerfile: ./apps/graphql-api/Dockerfile
    container_name: noda_turbo_graphql_api
    restart: always
    ports:
      - "4000:4000"
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      DATABASE_URL: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}"
      REDIS_URL: "redis://redis:6379"
      NODE_ENV: development
      PORT: 4000

  # --- NEW: LLM Chat Service ---
  llm-service:
    build:
      context: .
      dockerfile: ./apps/llm-service/Dockerfile
    container_name: noda_turbo_llm_service
    restart: always
    ports:
      # Expose the service on a new port to avoid conflicts
      - "5001:5001" 
    depends_on:
      db:
        condition: service_healthy
    environment:
      # This service will also need to connect to the database for context
      DATABASE_URL: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}"
      PORT: 5001
      # In a real app, you would add your LLM API Key here
      # GEMINI_API_KEY: "your_api_key_here"

volumes:
  db_data:
  redis_data:
  llm_service_data: